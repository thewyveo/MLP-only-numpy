{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np      # the ONLY library we used in this implementation is numpy.\n",
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Initialize Adam optimizer\n",
    "        \n",
    "        Args:\n",
    "            learning_rate: Step size for updates\n",
    "            beta1: Exponential decay rate for first moment estimates\n",
    "            beta2: Exponential decay rate for second moment estimates\n",
    "            epsilon: Small constant to prevent division by zero\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.t = 0             # time step\n",
    "        self.m_weights = None  # moment initialization for weights & biases\n",
    "        self.v_weights = None\n",
    "        self.m_biases = None\n",
    "        self.v_biases= None\n",
    "        \n",
    "    def initialize(self, weights, biases):\n",
    "        \"\"\"Initialize moment estimates based on model's weights and biases\"\"\"\n",
    "        self.m_weights = [np.zeros_like(w) for w in weights]    # first moment estimate (weights)\n",
    "        self.v_weights = [np.zeros_like(w) for w in weights]    # second moment estimate (weights)\n",
    "        self.m_biases = [np.zeros_like(b) for b in biases]      # first moment estimate (biases)\n",
    "        self.v_biases = [np.zeros_like(b) for b in biases]      # second moment estimate (biases)\n",
    "        \n",
    "    def update(self, weights, biases, weight_gradients, bias_gradients):\n",
    "        \"\"\"Update weights and biases using Adam optimization\"\"\"\n",
    "        if self.m_weights is None:      # initialize moment estimates after first update\n",
    "            self.initialize(weights, biases)\n",
    "            \n",
    "        self.t += 1\n",
    "        lr_t = self.learning_rate * np.sqrt(1 - self.beta2 ** self.t) / (1 - self.beta1 ** self.t)\n",
    "        \n",
    "        updated_weights = []\n",
    "        updated_biases = []\n",
    "        \n",
    "        # iterate over layers\n",
    "        for i in range(len(weights)):\n",
    "            # update weights\n",
    "            self.m_weights[i] = self.beta1 * self.m_weights[i] + (1 - self.beta1) * weight_gradients[i]\n",
    "            self.v_weights[i] = self.beta2 * self.v_weights[i] + (1 - self.beta2) * np.square(weight_gradients[i])\n",
    "            \n",
    "            # update weights with combined step\n",
    "            updated_w = weights[i] - lr_t * self.m_weights[i] / (np.sqrt(self.v_weights[i]) + self.epsilon)\n",
    "            updated_weights.append(updated_w)\n",
    "            \n",
    "            # update biases\n",
    "            self.m_biases[i] = self.beta1 * self.m_biases[i] + (1 - self.beta1) * bias_gradients[i]\n",
    "            self.v_biases[i] = self.beta2 * self.v_biases[i] + (1 - self.beta2) * np.square(bias_gradients[i])\n",
    "            \n",
    "            # update biases with combined step\n",
    "            updated_b = biases[i] - lr_t * self.m_biases[i] / (np.sqrt(self.v_biases[i]) + self.epsilon)\n",
    "            updated_biases.append(updated_b)\n",
    "            \n",
    "        return updated_weights, updated_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size=1, optimizer=None, learning_rate=0.01, gdescent='minibatch', metric_for_selection='accuracy', **optimizer_params):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Xavier initialization (modified to be a middleground between Xavier and He initialization, to be applicable to ReLU and Sigmoid)\n",
    "        # The logic here is that normal Xavier initialization is defined as (1 / nin+nout),\n",
    "        # and He initialization is defined as (2 / nin).\n",
    "        # our initialization formula, (2 / (nin + nout)), is a middleground between the two.\n",
    "        # this ensures that the weights are not too small (like Xavier) or too large (like He) - and that our activations are in the sweet spot.\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        self.weights = [\n",
    "            np.random.randn(layer_sizes[i], layer_sizes[i + 1]) \n",
    "            * np.sqrt(2.0 / (layer_sizes[i] + layer_sizes[i + 1]))\n",
    "            for i in range(len(layer_sizes) - 1)\n",
    "        ]\n",
    "        self.biases = [np.zeros((1, size)) for size in layer_sizes[1:]]\n",
    "        \n",
    "        # optimizer setup\n",
    "        if optimizer is not None:\n",
    "            self.optimizer_name = optimizer.lower()\n",
    "            if self.optimizer_name == 'adam':\n",
    "                self.optimizer = AdamOptimizer(learning_rate=learning_rate, **optimizer_params)\n",
    "        else:\n",
    "            # no optimizer option, this uses either stochastic GD or minibatch GD.\n",
    "            self.optimizer = None\n",
    "            self.optimizer_name = None\n",
    "\n",
    "        # gradient descent type\n",
    "        self.gdescent = gdescent\n",
    "\n",
    "        # metric for selection from validation set\n",
    "        self.metric_for_selection = metric_for_selection\n",
    "\n",
    "    # Sigmoid activation is used for the output layer, as it is a binary classification problem.\n",
    "    # Sigmoid also has the benefit of fitting values between 0 and 1, letting us calculate probabilities.\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -50, 50)))\n",
    "\n",
    "    def sigmoid_derivative(self, sigmoid_output):\n",
    "        return sigmoid_output * (1 - sigmoid_output)\n",
    "        \n",
    "    # ReLU activation is used for hidden layers, maintaining efficient gradient propagation.\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    # The loss function was chosen to be BCE as it is used for binary classification problems, and works with sigmoid well.\n",
    "    # The initial loss function was set to be MSE, but was changed to BCE to better fit the problem as it provides\n",
    "    # stronger gradients and better convergence.   \n",
    "    def binary_cross_entropy(self, y_true, y_pred):\n",
    "        \"\"\"Binary cross-entropy loss function\"\"\"\n",
    "        epsilon = 1e-15     # to prevent log(0) = -inf\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass with ReLU for hidden layers and sigmoid for output\"\"\"\n",
    "        self.layer_inputs = []  # store initial inputs w/out activation\n",
    "        self.activations = [X]\n",
    "        \n",
    "        # hidden layers with ReLU\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = self.activations[-1].dot(self.weights[i]) + self.biases[i]\n",
    "            self.layer_inputs.append(z)\n",
    "            a = self.relu(z)\n",
    "            self.activations.append(a)\n",
    "        \n",
    "        # output layer with sigmoid\n",
    "        z_out = self.activations[-1].dot(self.weights[-1]) + self.biases[-1]\n",
    "        self.layer_inputs.append(z_out)\n",
    "        output = self.sigmoid(z_out)\n",
    "        self.activations.append(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        \"\"\"Backward pass with proper derivatives based on activations\"\"\"\n",
    "        m = X.shape[0]  # batch size (depending on GD type, this can be 1 or batch_size)\n",
    "        \n",
    "        # compute gradient of BCE w.r.t. output\n",
    "        if self.output_size == 1:  # Binary classification\n",
    "            delta_output = output - y\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only binary classification is supported\")\n",
    "            # for multi-class, would need softmax+cross-entropy handling\n",
    "            # we don't use multi-class in this model, therefore this is a placeholder and we just leave it unimplemented.\n",
    "        \n",
    "        deltas = [delta_output]\n",
    "        \n",
    "        # backpropagate through hidden layers (backwards, so starting from last hidden layer)\n",
    "        for i in range(len(self.weights) - 1, 0, -1):\n",
    "            # compute delta for current layer via delta = delta(l+1) * weights(l+1)^T * relu_derivative(l)\n",
    "            delta = deltas[-1].dot(self.weights[i].T) * self.relu_derivative(self.layer_inputs[i-1])\n",
    "            deltas.append(delta)\n",
    "        \n",
    "        # reverse deltas to match layer order\n",
    "        deltas.reverse()\n",
    "        \n",
    "        #Â initialize lists for gradients\n",
    "        weight_gradients = []\n",
    "        bias_gradients = []\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            # gradient calculation with normalization by batch size\n",
    "            weight_grad = self.activations[i].T.dot(deltas[i]) / m\n",
    "            bias_grad = np.sum(deltas[i], axis=0, keepdims=True) / m\n",
    "            \n",
    "            weight_gradients.append(weight_grad)\n",
    "            bias_gradients.append(bias_grad)\n",
    "        \n",
    "        # update weights and biases with adam if toggled\n",
    "        if self.optimizer_name == 'adam':\n",
    "            self.weights, self.biases = self.optimizer.update(\n",
    "                self.weights, self.biases, weight_gradients, bias_gradients\n",
    "            )\n",
    "        else:\n",
    "            # standard gradient descent otherwise (either minibatch or stochastic)\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] -= weight_gradients[i] * self.learning_rate\n",
    "                self.biases[i] -= bias_gradients[i] * self.learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs=100, batch_size=16, patience=5):\n",
    "        \"\"\"Train the model with early stopping\"\"\"\n",
    "        n_samples = X.shape[0]  # number of samples\n",
    "        patience_counter = 0    # patience counter for early stopping\n",
    "        best_metric = 0       # initializations\n",
    "        best_weights = None\n",
    "        best_biases = None\n",
    "        \n",
    "        # training phase\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            epoch_losses = []\n",
    "            \n",
    "            if self.gdescent == 'minibatch':    # minibatch gradient descent\n",
    "                for i in range(0, n_samples, batch_size):\n",
    "                    X_batch = X_shuffled[i:i+batch_size]\n",
    "                    y_batch = y_shuffled[i:i+batch_size]\n",
    "                    \n",
    "                    output = self.forward(X_batch)  #forward\n",
    "                    batch_loss = self.binary_cross_entropy(y_batch, output) #loss\n",
    "                    epoch_losses.append(batch_loss) #append loss\n",
    "                    self.backward(X_batch, y_batch, output) #backward\n",
    "            else:                               # stochastic gradient descent\n",
    "                for i in range(n_samples):\n",
    "                    X_sample = X_shuffled[i:i+1]\n",
    "                    y_sample = y_shuffled[i:i+1]\n",
    "                    \n",
    "                    output = self.forward(X_sample) #same as above, but for each sample instead of batch\n",
    "                    sample_loss = self.binary_cross_entropy(y_sample, output)\n",
    "                    epoch_losses.append(sample_loss)\n",
    "                    self.backward(X_sample, y_sample, output)\n",
    "            \n",
    "            # calculate metrics\n",
    "            metrics = self.evaluate(X, y)\n",
    "            \n",
    "            # check for early stopping depending on whether metric keeps improving\n",
    "            if metrics[self.metric_for_selection] > best_metric:\n",
    "                best_metric = metrics[self.metric_for_selection]\n",
    "                best_weights = [w.copy() for w in self.weights]\n",
    "                best_biases = [b.copy() for b in self.biases]\n",
    "                patience_counter = 0    #if so, patience_counter is repeatedly set to 0, and the loop continues\n",
    "            else:\n",
    "                patience_counter += 1   #if improvement stops improving, patience_counter is incremented by 1\n",
    "            \n",
    "            if patience_counter >= patience:    #when this counter hits its threshold, the loop breaks\n",
    "                self.weights = best_weights     #and the best weights and biases are kept\n",
    "                self.biases = best_biases\n",
    "                break\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions with threshold as 0.5\"\"\"\n",
    "        output = self.forward(X)\n",
    "        return (output > 0.5).astype(int)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Evaluate the model's performance using accuracy, precision, recall, and F1-score.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X).flatten() #flatten to match y's shape\n",
    "        y = y.flatten() #flatten to match predictions' shape, as they need to be the same shape for comparison\n",
    "\n",
    "        # calculate all metrics\n",
    "        TP = np.sum((predictions == 1) & (y == 1))\n",
    "        FP = np.sum((predictions == 1) & (y == 0))\n",
    "        TN = np.sum((predictions == 0) & (y == 0))\n",
    "        FN = np.sum((predictions == 0) & (y == 1))\n",
    "        accuracy = (TP + TN) / (TP + FP + TN + FN) if (TP + FP + TN + FN) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1_score = (\n",
    "            (2 * precision * recall) / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"confusion_matrix\": {\n",
    "                \"TP\": TP, \"FP\": FP, \n",
    "                \"TN\": TN, \"FN\": FN\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def get_feature_importance(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate feature importance by measuring how much each feature\n",
    "        affects the prediction when perturbed\n",
    "        \"\"\"\n",
    "        base_preds = self.predict(X)                            # model's predictions on the original data\n",
    "        base_acc = np.mean(base_preds.flatten() == y.flatten()) # accuracy of the model on the original data\n",
    "        \n",
    "        importances = []\n",
    "        \n",
    "        for i in range(X.shape[1]):     # iterate over each feature, X.shape[1] is the number of features\n",
    "            X_shuffled = X.copy()       # create a copy\n",
    "            X_shuffled[:, i] = np.random.permutation(X_shuffled[:, i])  # random shuffle on the i-th feature (random permutation)\n",
    "            \n",
    "            preds_shuffled = self.predict(X_shuffled)   # model's predictions on the dataset with the i-th feature shuffled\n",
    "            acc_shuffled = np.mean(preds_shuffled.flatten() == y.flatten()) # and its accuracy\n",
    "            \n",
    "            importance = base_acc - acc_shuffled    # importance is calculated as the decrease in accuracy when feature is shuffled\n",
    "            importances.append(importance)          # append to the list\n",
    "            \n",
    "        return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Preprocesses the diabetes dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # define encoding for categorical features (in this case, only gender)\n",
    "    encoding_dicts = {\n",
    "        \"gender\": {\"Female\": 0, \"Male\": 1},\n",
    "    }\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    # iterate through each line of the dataset\n",
    "    for line in data.strip().split(\"\\n\")[1:]:\n",
    "        features = line.strip().split(\",\")\n",
    "        \n",
    "        y_data.append(int(features[-1]))    # extract target value (diabetes --> 0 or 1)\n",
    "        \n",
    "        encoded_features = []   # initialize encoded features list\n",
    "        \n",
    "        # gender encoding (categorical)\n",
    "        gender = features[0]  # (gender is the first feature, hence 0)\n",
    "        encoded_features.append(encoding_dicts[\"gender\"].get(gender, -1))  # encode gender\n",
    "        # NOTE: we do not apply one-hot encoding to gender, as the values are only in binary (0 or 1) and\n",
    "        # since we're working with a NN, it can handle binary values well - therefore even though the\n",
    "        # gender is encoded \"numerically\" it is treated as a categorical feature.\n",
    "        # (i.e. even though it is not [1, 0] for 0 and [0, 1] for 1; it is still categorical)\n",
    "        \n",
    "        # numerical features are: age, hypertension, heart_disease, bmi, HbA1c_level, blood_glucose_level\n",
    "        numerical_features = features[1:-1]  # all features except gender and diabetes target\n",
    "        encoded_features.extend([float(x) for x in numerical_features])  # convert to float\n",
    "        \n",
    "        X_data.append(encoded_features) # and append to the X_data list\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X_data, dtype=float)\n",
    "    y = np.array(y_data, dtype=float).reshape(-1, 1)\n",
    "    \n",
    "    # normalize only the numerical data, excluding the first categorical feature\n",
    "    numerical_indices = list(range(1, X.shape[1]))  # indexes for numerical features, we skip the first feature (gender)\n",
    "    mean = np.mean(X[:, numerical_indices], axis=0) # mean and std are calculated only on the numerical features with indexing\n",
    "    std = np.std(X[:, numerical_indices], axis=0)\n",
    "    std[std < 1e-5] = 1  # prevent division by zero\n",
    "    \n",
    "    # normalize the data\n",
    "    X_normalized = X.copy()\n",
    "    X_normalized[:, numerical_indices] = (X[:, numerical_indices] - mean) / std\n",
    "    \n",
    "    # set data split sizes\n",
    "    n_samples = len(X)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    train_size = int(n_samples * train_ratio)         # the training set size is set to be train_ratio\n",
    "    val_size = int(n_samples * (1 - train_ratio) / 2) # the / 2 is to split the remaining data into validation and test sets\n",
    "    train_indices = indices[:train_size]              # e.g. if train_ratio = 0.8, then the rest is split into another 0.1 and 0.1\n",
    "    val_indices = indices[train_size : train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "    \n",
    "    # execute the split\n",
    "    X_train, y_train = X_normalized[train_indices], y[train_indices]\n",
    "    X_val, y_val = X_normalized[val_indices], y[val_indices]\n",
    "    X_test, y_test = X_normalized[test_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification(data, \n",
    "                      hidden_sizes_options=[[16], [32], [64], [32, 16], [64, 32], [64, 32, 16]],\n",
    "                      learning_rate_options=[0.01, 0.005, 0.001],\n",
    "                      optimizer='adam',\n",
    "                      epochs=50,\n",
    "                      batch_size=32,\n",
    "                      gdescent='minibatch',\n",
    "                      patience=5,\n",
    "                      random_seed=None,\n",
    "                      train_ratio = 0.8,\n",
    "                      metric_for_selection='accuracy',  # Better for imbalanced data\n",
    "                      **optimizer_params):\n",
    "    \"\"\"\n",
    "    Run diabetes classification with hyperparameter tuning\n",
    "    \n",
    "    Args:\n",
    "        data: String containing the diabetes dataset\n",
    "        hidden_sizes_options: List of hidden layer configurations to try\n",
    "        learning_rate_options: List of learning rates to try\n",
    "        optimizer: Optimization algorithm ('adam' or None for SGD)\n",
    "        epochs: Maximum number of training epochs\n",
    "        batch_size: Batch size for mini-batch gradient descent\n",
    "        gdescent: Type of gradient descent ('minibatch' or 'stochastic')\n",
    "        patience: Number of epochs to wait before early stopping\n",
    "        validation_split: Proportion of training data to use for validation\n",
    "        random_seed: Random seed for reproducibility\n",
    "        metric_for_selection: Metric to use for model selection ('accuracy', 'f1_score', etc.)\n",
    "        optimizer_params: Additional parameters for the optimizer\n",
    "        \n",
    "    Returns:\n",
    "        best_model: The best performing model\n",
    "        all_results: Complete results for all hyperparameter combinations\n",
    "    \"\"\"\n",
    "\n",
    "    # set seed if provided in params, otherwise defaults to None\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "        print(f\"Random seed set to {random_seed}\")\n",
    "    \n",
    "    # preprocess data\n",
    "    print(\"Preprocessing data...\")\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val, X_orig, y_orig = preprocess_data(data, train_ratio=0.8)\n",
    "    input_size = X_train.shape[1]\n",
    "    print()\n",
    "    \n",
    "    # store results for each hyperparameter combination\n",
    "    results = []\n",
    "    \n",
    "    print(\"Starting hyperparameter selection via grid-search, via a validation set...\")\n",
    "    print(f\"Using {metric_for_selection} for model selection...\")\n",
    "    \n",
    "    total_combinations = len(hidden_sizes_options) * len(learning_rate_options)\n",
    "    iterations = 0\n",
    "    # grid-search over hyperparameters\n",
    "    for hidden_sizes in hidden_sizes_options:\n",
    "        for lr in learning_rate_options:\n",
    "            \n",
    "            # define a model with current hyperparameters\n",
    "            model = MLP(input_size, \n",
    "                      hidden_sizes, \n",
    "                      optimizer=optimizer, \n",
    "                      learning_rate=lr, \n",
    "                      gdescent=gdescent,\n",
    "                      metric_for_selection = metric_for_selection, \n",
    "                      **optimizer_params)\n",
    "            \n",
    "            # train with early stopping for each model\n",
    "            model.train(X_train, y_train, \n",
    "                       epochs=epochs, \n",
    "                       batch_size=batch_size, \n",
    "                       patience=patience)\n",
    "            \n",
    "            # evaluate on validation set\n",
    "            validation_metrics = model.evaluate(X_val, y_val)\n",
    "            \n",
    "            results.append({\n",
    "                'hidden_sizes': hidden_sizes,\n",
    "                'learning_rate': lr,\n",
    "                'model': model,\n",
    "                'validation_metrics': validation_metrics\n",
    "            })\n",
    "\n",
    "            iterations += 1\n",
    "            if iterations % 3 == 0:\n",
    "                print(f\"    Completed {iterations}/{total_combinations} iterations\")\n",
    "    print(\"Hyperparameter selection via grid-search complete.\")\n",
    "    \n",
    "    # find best model based on selected metric\n",
    "    best_result = max(results, key=lambda x: x['validation_metrics'][metric_for_selection])\n",
    "    best_model = best_result['model']\n",
    "    \n",
    "    print()\n",
    "    print(\"Best hyperparameters:\")\n",
    "    print(f\"    Hidden sizes: {best_result['hidden_sizes']}\")\n",
    "    print(f\"    Learning rate: {best_result['learning_rate']}\")\n",
    "    print(f\"    Validation {metric_for_selection}: {best_result['validation_metrics'][metric_for_selection]:.4f}\")\n",
    "    \n",
    "    # evaluate best model on test set (final evaluation)\n",
    "    test_metrics = best_model.evaluate(X_test, y_test)\n",
    "    print(\"\\nTest Set Results with best model:\")\n",
    "    print(f\"    Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"    Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"    Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"    F1-Score: {test_metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    # get feature importance (if applicable)\n",
    "    try:\n",
    "        importances = best_model.get_feature_importance(X_train, y_train)   # call the get_feature_importance method on the best model\n",
    "        features = [\"Gender\", \"Age\", \"Hypertension\", \"Heart Disease\", \"BMI\", \"HbA1c\", \"Blood Glucose\"]\n",
    "        \n",
    "        print(\"\\nFeature Importance:\")\n",
    "        # iterating over tuples of (feature, importance) that are zipped together, and sorting by importance\n",
    "        for i, (feature, importance) in enumerate(sorted(zip(features, importances), key=lambda x: abs(x[1]), reverse=True)):\n",
    "            print(f\"    {i+1}. {feature}: {importance:.4f}\")\n",
    "    except:\n",
    "        print(\"\\nFeature importance calculation not available\")\n",
    "    \n",
    "    return best_model, results  # this return is a placeholder, we don't actually use it for anything.\n",
    "                                # however it could possibly be used to store the model, or to analyze results for further insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "\n",
      "Starting hyperparameter selection via grid-search, via a validation set...\n",
      "Using accuracy for model selection...\n",
      "    Completed 3/18 iterations\n",
      "    Completed 6/18 iterations\n",
      "    Completed 9/18 iterations\n",
      "    Completed 12/18 iterations\n",
      "    Completed 15/18 iterations\n",
      "    Completed 18/18 iterations\n",
      "Hyperparameter selection via grid-search complete.\n",
      "\n",
      "Best hyperparameters:\n",
      "    Hidden sizes: [64, 32, 16]\n",
      "    Learning rate: 0.005\n",
      "    Validation accuracy: 0.9082\n",
      "\n",
      "Test Set Results with best model:\n",
      "    Accuracy: 0.9077\n",
      "    Precision: 0.8883\n",
      "    Recall: 0.9331\n",
      "    F1-Score: 0.9101\n",
      "\n",
      "Feature Importance:\n",
      "    1. HbA1c: 0.1947\n",
      "    2. Blood Glucose: 0.1267\n",
      "    3. Age: 0.0468\n",
      "    4. BMI: 0.0181\n",
      "    5. Hypertension: 0.0098\n",
      "    6. Heart Disease: 0.0087\n",
      "    7. Gender: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "with open(\"diabetes-processed.csv\", \"r\") as file:\n",
    "    data = file.read()\n",
    "    \n",
    "X_train, y_train, X_test, y_test, X_val, y_val, _, _ = preprocess_data(data)\n",
    "#create_baseline_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# run the code\n",
    "model, results = run_classification(data, optimizer='adam', gdescent='minibatch', random_seed=None, metric_for_selection='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLAUDE TEST ETTI \n",
    "\n",
    "COMPARISON OF OPTIMIZATION METHODS\n",
    "\n",
    "Adam + Minibatch Accuracy: 0.9024\n",
    "\n",
    "SGD (Stochastic) Accuracy: 0.9001\n",
    "\n",
    "Minibatch SGD Accuracy: 0.9012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING WITH ADAM + MINIBATCH\n",
      "==================================================\n",
      "Epoch 0, Train Loss: 0.3959, Train Acc: 0.8829, Val Loss: 0.2609, Val Acc: 0.8868\n",
      "Epoch 1, Train Loss: 0.2566, Train Acc: 0.8849, Val Loss: 0.2512, Val Acc: 0.8857\n",
      "Epoch 2, Train Loss: 0.2511, Train Acc: 0.8854, Val Loss: 0.2489, Val Acc: 0.8857\n",
      "Epoch 3, Train Loss: 0.2479, Train Acc: 0.8851, Val Loss: 0.2482, Val Acc: 0.8831\n",
      "Epoch 4, Train Loss: 0.2459, Train Acc: 0.8882, Val Loss: 0.2453, Val Acc: 0.8835\n",
      "Epoch 5, Train Loss: 0.2437, Train Acc: 0.8885, Val Loss: 0.2435, Val Acc: 0.8857\n",
      "Epoch 6, Train Loss: 0.2418, Train Acc: 0.8887, Val Loss: 0.2423, Val Acc: 0.8860\n",
      "Epoch 7, Train Loss: 0.2391, Train Acc: 0.8888, Val Loss: 0.2420, Val Acc: 0.8890\n",
      "Epoch 8, Train Loss: 0.2369, Train Acc: 0.8901, Val Loss: 0.2385, Val Acc: 0.8871\n",
      "Epoch 9, Train Loss: 0.2333, Train Acc: 0.8898, Val Loss: 0.2345, Val Acc: 0.8886\n",
      "Epoch 10, Train Loss: 0.2296, Train Acc: 0.8915, Val Loss: 0.2319, Val Acc: 0.8904\n",
      "Epoch 11, Train Loss: 0.2257, Train Acc: 0.8938, Val Loss: 0.2283, Val Acc: 0.8930\n",
      "Epoch 12, Train Loss: 0.2217, Train Acc: 0.8936, Val Loss: 0.2248, Val Acc: 0.8930\n",
      "Epoch 13, Train Loss: 0.2178, Train Acc: 0.8955, Val Loss: 0.2196, Val Acc: 0.8945\n",
      "Epoch 14, Train Loss: 0.2138, Train Acc: 0.8970, Val Loss: 0.2170, Val Acc: 0.8989\n",
      "Epoch 15, Train Loss: 0.2108, Train Acc: 0.8994, Val Loss: 0.2144, Val Acc: 0.8993\n",
      "Epoch 16, Train Loss: 0.2078, Train Acc: 0.8997, Val Loss: 0.2114, Val Acc: 0.8974\n",
      "Epoch 17, Train Loss: 0.2055, Train Acc: 0.9017, Val Loss: 0.2087, Val Acc: 0.9004\n",
      "Epoch 18, Train Loss: 0.2035, Train Acc: 0.9028, Val Loss: 0.2070, Val Acc: 0.9004\n",
      "Epoch 19, Train Loss: 0.2015, Train Acc: 0.9028, Val Loss: 0.2065, Val Acc: 0.8978\n",
      "Epoch 20, Train Loss: 0.2000, Train Acc: 0.9034, Val Loss: 0.2055, Val Acc: 0.8989\n",
      "Epoch 21, Train Loss: 0.1988, Train Acc: 0.9038, Val Loss: 0.2029, Val Acc: 0.9022\n",
      "Epoch 22, Train Loss: 0.1976, Train Acc: 0.9041, Val Loss: 0.2018, Val Acc: 0.9000\n",
      "Epoch 23, Train Loss: 0.1965, Train Acc: 0.9052, Val Loss: 0.1997, Val Acc: 0.9018\n",
      "Epoch 24, Train Loss: 0.1956, Train Acc: 0.9054, Val Loss: 0.2001, Val Acc: 0.9004\n",
      "Epoch 25, Train Loss: 0.1949, Train Acc: 0.9054, Val Loss: 0.1986, Val Acc: 0.9011\n",
      "Epoch 26, Train Loss: 0.1941, Train Acc: 0.9051, Val Loss: 0.1982, Val Acc: 0.9040\n",
      "Epoch 27, Train Loss: 0.1934, Train Acc: 0.9053, Val Loss: 0.1966, Val Acc: 0.9033\n",
      "Epoch 28, Train Loss: 0.1928, Train Acc: 0.9059, Val Loss: 0.1962, Val Acc: 0.9040\n",
      "Epoch 29, Train Loss: 0.1922, Train Acc: 0.9076, Val Loss: 0.1961, Val Acc: 0.9044\n",
      "Epoch 30, Train Loss: 0.1917, Train Acc: 0.9059, Val Loss: 0.1963, Val Acc: 0.9044\n",
      "Epoch 31, Train Loss: 0.1914, Train Acc: 0.9066, Val Loss: 0.1959, Val Acc: 0.9040\n",
      "Epoch 32, Train Loss: 0.1905, Train Acc: 0.9077, Val Loss: 0.1949, Val Acc: 0.9040\n",
      "Epoch 33, Train Loss: 0.1900, Train Acc: 0.9073, Val Loss: 0.1939, Val Acc: 0.9062\n",
      "Epoch 34, Train Loss: 0.1893, Train Acc: 0.9071, Val Loss: 0.1963, Val Acc: 0.9033\n",
      "Epoch 35, Train Loss: 0.1892, Train Acc: 0.9084, Val Loss: 0.1935, Val Acc: 0.9051\n",
      "Epoch 36, Train Loss: 0.1886, Train Acc: 0.9083, Val Loss: 0.1932, Val Acc: 0.9048\n",
      "Epoch 37, Train Loss: 0.1882, Train Acc: 0.9085, Val Loss: 0.1920, Val Acc: 0.9048\n",
      "Epoch 38, Train Loss: 0.1881, Train Acc: 0.9085, Val Loss: 0.1917, Val Acc: 0.9055\n",
      "Epoch 39, Train Loss: 0.1874, Train Acc: 0.9091, Val Loss: 0.1928, Val Acc: 0.9055\n",
      "Epoch 40, Train Loss: 0.1873, Train Acc: 0.9086, Val Loss: 0.1912, Val Acc: 0.9048\n",
      "Epoch 41, Train Loss: 0.1869, Train Acc: 0.9084, Val Loss: 0.1919, Val Acc: 0.9051\n",
      "Epoch 42, Train Loss: 0.1867, Train Acc: 0.9086, Val Loss: 0.1920, Val Acc: 0.9055\n",
      "Epoch 43, Train Loss: 0.1865, Train Acc: 0.9074, Val Loss: 0.1910, Val Acc: 0.9048\n",
      "Epoch 44, Train Loss: 0.1864, Train Acc: 0.9090, Val Loss: 0.1895, Val Acc: 0.9040\n",
      "Epoch 45, Train Loss: 0.1860, Train Acc: 0.9084, Val Loss: 0.1909, Val Acc: 0.9051\n",
      "Epoch 46, Train Loss: 0.1852, Train Acc: 0.9069, Val Loss: 0.1904, Val Acc: 0.9026\n",
      "Epoch 47, Train Loss: 0.1858, Train Acc: 0.9099, Val Loss: 0.1924, Val Acc: 0.9040\n",
      "Epoch 48, Train Loss: 0.1853, Train Acc: 0.9089, Val Loss: 0.1895, Val Acc: 0.9059\n",
      "Epoch 49, Train Loss: 0.1855, Train Acc: 0.9098, Val Loss: 0.1905, Val Acc: 0.9029\n",
      "Epoch 50, Train Loss: 0.1851, Train Acc: 0.9078, Val Loss: 0.1906, Val Acc: 0.9040\n",
      "Epoch 51, Train Loss: 0.1849, Train Acc: 0.9091, Val Loss: 0.1884, Val Acc: 0.9044\n",
      "Epoch 52, Train Loss: 0.1848, Train Acc: 0.9092, Val Loss: 0.1897, Val Acc: 0.9037\n",
      "Epoch 53, Train Loss: 0.1847, Train Acc: 0.9085, Val Loss: 0.1883, Val Acc: 0.9048\n",
      "Epoch 54, Train Loss: 0.1838, Train Acc: 0.9117, Val Loss: 0.1901, Val Acc: 0.9051\n",
      "Epoch 55, Train Loss: 0.1840, Train Acc: 0.9089, Val Loss: 0.1897, Val Acc: 0.9048\n",
      "Epoch 56, Train Loss: 0.1844, Train Acc: 0.9090, Val Loss: 0.1885, Val Acc: 0.9011\n",
      "Epoch 57, Train Loss: 0.1842, Train Acc: 0.9096, Val Loss: 0.1893, Val Acc: 0.9044\n",
      "Early stopping at epoch 58\n",
      "\n",
      "Adam + Minibatch Test Results:\n",
      "Accuracy: 0.9024\n",
      "F1-Score: 0.9068\n",
      "\n",
      "==================================================\n",
      "TRAINING WITH SGD\n",
      "==================================================\n",
      "Epoch 0, Train Loss: 0.2654, Train Acc: 0.8868, Val Loss: 0.2496, Val Acc: 0.8809\n",
      "Epoch 1, Train Loss: 0.2412, Train Acc: 0.8859, Val Loss: 0.2404, Val Acc: 0.8882\n",
      "Epoch 2, Train Loss: 0.2307, Train Acc: 0.8895, Val Loss: 0.2305, Val Acc: 0.8886\n",
      "Epoch 3, Train Loss: 0.2153, Train Acc: 0.9019, Val Loss: 0.2050, Val Acc: 0.9004\n",
      "Epoch 4, Train Loss: 0.2051, Train Acc: 0.8944, Val Loss: 0.2076, Val Acc: 0.8912\n",
      "Epoch 5, Train Loss: 0.1998, Train Acc: 0.9067, Val Loss: 0.1963, Val Acc: 0.9059\n",
      "Epoch 6, Train Loss: 0.1967, Train Acc: 0.9065, Val Loss: 0.1941, Val Acc: 0.9103\n",
      "Epoch 7, Train Loss: 0.1954, Train Acc: 0.9044, Val Loss: 0.1971, Val Acc: 0.9048\n",
      "Epoch 8, Train Loss: 0.1939, Train Acc: 0.9050, Val Loss: 0.1951, Val Acc: 0.9033\n",
      "Epoch 9, Train Loss: 0.1935, Train Acc: 0.9052, Val Loss: 0.1979, Val Acc: 0.8963\n",
      "Epoch 10, Train Loss: 0.1925, Train Acc: 0.9088, Val Loss: 0.1910, Val Acc: 0.9107\n",
      "Epoch 11, Train Loss: 0.1913, Train Acc: 0.9058, Val Loss: 0.1919, Val Acc: 0.9062\n",
      "Epoch 12, Train Loss: 0.1904, Train Acc: 0.9088, Val Loss: 0.1890, Val Acc: 0.9136\n",
      "Epoch 13, Train Loss: 0.1910, Train Acc: 0.9091, Val Loss: 0.1902, Val Acc: 0.9070\n",
      "Epoch 14, Train Loss: 0.1898, Train Acc: 0.9089, Val Loss: 0.1859, Val Acc: 0.9099\n",
      "Epoch 15, Train Loss: 0.1890, Train Acc: 0.9108, Val Loss: 0.1875, Val Acc: 0.9129\n",
      "Epoch 16, Train Loss: 0.1881, Train Acc: 0.9079, Val Loss: 0.1957, Val Acc: 0.9081\n",
      "Epoch 17, Train Loss: 0.1884, Train Acc: 0.9099, Val Loss: 0.1882, Val Acc: 0.9088\n",
      "Epoch 18, Train Loss: 0.1882, Train Acc: 0.9039, Val Loss: 0.1911, Val Acc: 0.9059\n",
      "Early stopping at epoch 19\n",
      "\n",
      "SGD Test Results:\n",
      "Accuracy: 0.9001\n",
      "F1-Score: 0.9062\n",
      "\n",
      "==================================================\n",
      "TRAINING WITH MINIBATCH SGD (WITHOUT ADAM)\n",
      "==================================================\n",
      "Epoch 0, Train Loss: 0.5051, Train Acc: 0.8547, Val Loss: 0.3734, Val Acc: 0.8460\n",
      "Epoch 1, Train Loss: 0.3141, Train Acc: 0.8780, Val Loss: 0.2937, Val Acc: 0.8665\n",
      "Epoch 2, Train Loss: 0.2688, Train Acc: 0.8848, Val Loss: 0.2748, Val Acc: 0.8728\n",
      "Epoch 3, Train Loss: 0.2563, Train Acc: 0.8863, Val Loss: 0.2693, Val Acc: 0.8750\n",
      "Epoch 4, Train Loss: 0.2519, Train Acc: 0.8869, Val Loss: 0.2668, Val Acc: 0.8735\n",
      "Epoch 5, Train Loss: 0.2496, Train Acc: 0.8869, Val Loss: 0.2654, Val Acc: 0.8743\n",
      "Epoch 6, Train Loss: 0.2481, Train Acc: 0.8869, Val Loss: 0.2645, Val Acc: 0.8728\n",
      "Epoch 7, Train Loss: 0.2472, Train Acc: 0.8862, Val Loss: 0.2636, Val Acc: 0.8750\n",
      "Epoch 8, Train Loss: 0.2464, Train Acc: 0.8869, Val Loss: 0.2630, Val Acc: 0.8739\n",
      "Epoch 9, Train Loss: 0.2456, Train Acc: 0.8858, Val Loss: 0.2629, Val Acc: 0.8728\n",
      "Epoch 10, Train Loss: 0.2451, Train Acc: 0.8871, Val Loss: 0.2619, Val Acc: 0.8724\n",
      "Epoch 11, Train Loss: 0.2444, Train Acc: 0.8870, Val Loss: 0.2615, Val Acc: 0.8724\n",
      "Epoch 12, Train Loss: 0.2439, Train Acc: 0.8870, Val Loss: 0.2611, Val Acc: 0.8724\n",
      "Epoch 13, Train Loss: 0.2434, Train Acc: 0.8863, Val Loss: 0.2602, Val Acc: 0.8750\n",
      "Epoch 14, Train Loss: 0.2429, Train Acc: 0.8869, Val Loss: 0.2598, Val Acc: 0.8761\n",
      "Epoch 15, Train Loss: 0.2425, Train Acc: 0.8859, Val Loss: 0.2594, Val Acc: 0.8768\n",
      "Epoch 16, Train Loss: 0.2420, Train Acc: 0.8862, Val Loss: 0.2588, Val Acc: 0.8776\n",
      "Epoch 17, Train Loss: 0.2415, Train Acc: 0.8864, Val Loss: 0.2584, Val Acc: 0.8761\n",
      "Epoch 18, Train Loss: 0.2411, Train Acc: 0.8873, Val Loss: 0.2584, Val Acc: 0.8743\n",
      "Epoch 19, Train Loss: 0.2407, Train Acc: 0.8871, Val Loss: 0.2575, Val Acc: 0.8768\n",
      "Epoch 20, Train Loss: 0.2403, Train Acc: 0.8873, Val Loss: 0.2574, Val Acc: 0.8757\n",
      "Epoch 21, Train Loss: 0.2398, Train Acc: 0.8869, Val Loss: 0.2571, Val Acc: 0.8746\n",
      "Epoch 22, Train Loss: 0.2394, Train Acc: 0.8879, Val Loss: 0.2562, Val Acc: 0.8772\n",
      "Epoch 23, Train Loss: 0.2391, Train Acc: 0.8877, Val Loss: 0.2557, Val Acc: 0.8768\n",
      "Epoch 24, Train Loss: 0.2388, Train Acc: 0.8882, Val Loss: 0.2553, Val Acc: 0.8779\n",
      "Epoch 25, Train Loss: 0.2384, Train Acc: 0.8879, Val Loss: 0.2550, Val Acc: 0.8779\n",
      "Epoch 26, Train Loss: 0.2380, Train Acc: 0.8883, Val Loss: 0.2546, Val Acc: 0.8779\n",
      "Epoch 27, Train Loss: 0.2376, Train Acc: 0.8892, Val Loss: 0.2539, Val Acc: 0.8776\n",
      "Epoch 28, Train Loss: 0.2372, Train Acc: 0.8897, Val Loss: 0.2534, Val Acc: 0.8787\n",
      "Epoch 29, Train Loss: 0.2368, Train Acc: 0.8899, Val Loss: 0.2530, Val Acc: 0.8779\n",
      "Epoch 30, Train Loss: 0.2365, Train Acc: 0.8897, Val Loss: 0.2531, Val Acc: 0.8757\n",
      "Epoch 31, Train Loss: 0.2360, Train Acc: 0.8892, Val Loss: 0.2526, Val Acc: 0.8768\n",
      "Epoch 32, Train Loss: 0.2354, Train Acc: 0.8893, Val Loss: 0.2520, Val Acc: 0.8790\n",
      "Epoch 33, Train Loss: 0.2352, Train Acc: 0.8903, Val Loss: 0.2520, Val Acc: 0.8776\n",
      "Epoch 34, Train Loss: 0.2348, Train Acc: 0.8891, Val Loss: 0.2506, Val Acc: 0.8787\n",
      "Epoch 35, Train Loss: 0.2345, Train Acc: 0.8897, Val Loss: 0.2506, Val Acc: 0.8776\n",
      "Epoch 36, Train Loss: 0.2340, Train Acc: 0.8894, Val Loss: 0.2499, Val Acc: 0.8779\n",
      "Epoch 37, Train Loss: 0.2337, Train Acc: 0.8905, Val Loss: 0.2497, Val Acc: 0.8790\n",
      "Epoch 38, Train Loss: 0.2333, Train Acc: 0.8906, Val Loss: 0.2496, Val Acc: 0.8794\n",
      "Epoch 39, Train Loss: 0.2329, Train Acc: 0.8910, Val Loss: 0.2490, Val Acc: 0.8801\n",
      "Epoch 40, Train Loss: 0.2324, Train Acc: 0.8895, Val Loss: 0.2482, Val Acc: 0.8812\n",
      "Epoch 41, Train Loss: 0.2322, Train Acc: 0.8908, Val Loss: 0.2478, Val Acc: 0.8798\n",
      "Epoch 42, Train Loss: 0.2318, Train Acc: 0.8900, Val Loss: 0.2475, Val Acc: 0.8809\n",
      "Epoch 43, Train Loss: 0.2314, Train Acc: 0.8909, Val Loss: 0.2476, Val Acc: 0.8805\n",
      "Epoch 44, Train Loss: 0.2309, Train Acc: 0.8912, Val Loss: 0.2472, Val Acc: 0.8812\n",
      "Epoch 45, Train Loss: 0.2308, Train Acc: 0.8906, Val Loss: 0.2466, Val Acc: 0.8820\n",
      "Epoch 46, Train Loss: 0.2303, Train Acc: 0.8906, Val Loss: 0.2459, Val Acc: 0.8816\n",
      "Epoch 47, Train Loss: 0.2299, Train Acc: 0.8911, Val Loss: 0.2458, Val Acc: 0.8824\n",
      "Epoch 48, Train Loss: 0.2296, Train Acc: 0.8910, Val Loss: 0.2455, Val Acc: 0.8812\n",
      "Epoch 49, Train Loss: 0.2292, Train Acc: 0.8907, Val Loss: 0.2449, Val Acc: 0.8838\n",
      "Epoch 50, Train Loss: 0.2288, Train Acc: 0.8912, Val Loss: 0.2446, Val Acc: 0.8816\n",
      "Epoch 51, Train Loss: 0.2283, Train Acc: 0.8914, Val Loss: 0.2442, Val Acc: 0.8824\n",
      "Epoch 52, Train Loss: 0.2278, Train Acc: 0.8920, Val Loss: 0.2439, Val Acc: 0.8831\n",
      "Epoch 53, Train Loss: 0.2276, Train Acc: 0.8910, Val Loss: 0.2436, Val Acc: 0.8816\n",
      "Epoch 54, Train Loss: 0.2270, Train Acc: 0.8921, Val Loss: 0.2436, Val Acc: 0.8846\n",
      "Epoch 55, Train Loss: 0.2268, Train Acc: 0.8925, Val Loss: 0.2440, Val Acc: 0.8790\n",
      "Epoch 56, Train Loss: 0.2265, Train Acc: 0.8922, Val Loss: 0.2430, Val Acc: 0.8812\n",
      "Epoch 57, Train Loss: 0.2259, Train Acc: 0.8916, Val Loss: 0.2422, Val Acc: 0.8827\n",
      "Epoch 58, Train Loss: 0.2257, Train Acc: 0.8922, Val Loss: 0.2426, Val Acc: 0.8827\n",
      "Epoch 59, Train Loss: 0.2253, Train Acc: 0.8923, Val Loss: 0.2426, Val Acc: 0.8812\n",
      "Epoch 60, Train Loss: 0.2249, Train Acc: 0.8929, Val Loss: 0.2419, Val Acc: 0.8831\n",
      "Epoch 61, Train Loss: 0.2246, Train Acc: 0.8933, Val Loss: 0.2417, Val Acc: 0.8831\n",
      "Epoch 62, Train Loss: 0.2241, Train Acc: 0.8928, Val Loss: 0.2408, Val Acc: 0.8827\n",
      "Epoch 63, Train Loss: 0.2239, Train Acc: 0.8935, Val Loss: 0.2405, Val Acc: 0.8831\n",
      "Epoch 64, Train Loss: 0.2236, Train Acc: 0.8941, Val Loss: 0.2402, Val Acc: 0.8860\n",
      "Epoch 65, Train Loss: 0.2234, Train Acc: 0.8937, Val Loss: 0.2402, Val Acc: 0.8842\n",
      "Epoch 66, Train Loss: 0.2231, Train Acc: 0.8946, Val Loss: 0.2400, Val Acc: 0.8846\n",
      "Epoch 67, Train Loss: 0.2226, Train Acc: 0.8939, Val Loss: 0.2394, Val Acc: 0.8842\n",
      "Epoch 68, Train Loss: 0.2224, Train Acc: 0.8938, Val Loss: 0.2391, Val Acc: 0.8838\n",
      "Epoch 69, Train Loss: 0.2219, Train Acc: 0.8944, Val Loss: 0.2392, Val Acc: 0.8853\n",
      "Epoch 70, Train Loss: 0.2218, Train Acc: 0.8950, Val Loss: 0.2386, Val Acc: 0.8838\n",
      "Epoch 71, Train Loss: 0.2213, Train Acc: 0.8957, Val Loss: 0.2387, Val Acc: 0.8849\n",
      "Epoch 72, Train Loss: 0.2211, Train Acc: 0.8947, Val Loss: 0.2379, Val Acc: 0.8846\n",
      "Epoch 73, Train Loss: 0.2208, Train Acc: 0.8950, Val Loss: 0.2378, Val Acc: 0.8846\n",
      "Epoch 74, Train Loss: 0.2204, Train Acc: 0.8955, Val Loss: 0.2377, Val Acc: 0.8849\n",
      "Epoch 75, Train Loss: 0.2202, Train Acc: 0.8941, Val Loss: 0.2383, Val Acc: 0.8838\n",
      "Epoch 76, Train Loss: 0.2198, Train Acc: 0.8953, Val Loss: 0.2368, Val Acc: 0.8853\n",
      "Epoch 77, Train Loss: 0.2196, Train Acc: 0.8948, Val Loss: 0.2374, Val Acc: 0.8827\n",
      "Epoch 78, Train Loss: 0.2192, Train Acc: 0.8958, Val Loss: 0.2359, Val Acc: 0.8857\n",
      "Epoch 79, Train Loss: 0.2190, Train Acc: 0.8964, Val Loss: 0.2362, Val Acc: 0.8857\n",
      "Epoch 80, Train Loss: 0.2187, Train Acc: 0.8956, Val Loss: 0.2367, Val Acc: 0.8827\n",
      "Epoch 81, Train Loss: 0.2184, Train Acc: 0.8960, Val Loss: 0.2358, Val Acc: 0.8853\n",
      "Epoch 82, Train Loss: 0.2183, Train Acc: 0.8967, Val Loss: 0.2352, Val Acc: 0.8849\n",
      "Epoch 83, Train Loss: 0.2178, Train Acc: 0.8962, Val Loss: 0.2349, Val Acc: 0.8860\n",
      "Epoch 84, Train Loss: 0.2176, Train Acc: 0.8968, Val Loss: 0.2348, Val Acc: 0.8849\n",
      "Epoch 85, Train Loss: 0.2172, Train Acc: 0.8976, Val Loss: 0.2351, Val Acc: 0.8846\n",
      "Epoch 86, Train Loss: 0.2170, Train Acc: 0.8980, Val Loss: 0.2343, Val Acc: 0.8871\n",
      "Epoch 87, Train Loss: 0.2168, Train Acc: 0.8977, Val Loss: 0.2341, Val Acc: 0.8853\n",
      "Epoch 88, Train Loss: 0.2164, Train Acc: 0.8978, Val Loss: 0.2339, Val Acc: 0.8853\n",
      "Epoch 89, Train Loss: 0.2159, Train Acc: 0.8974, Val Loss: 0.2334, Val Acc: 0.8893\n",
      "Epoch 90, Train Loss: 0.2157, Train Acc: 0.8971, Val Loss: 0.2327, Val Acc: 0.8882\n",
      "Epoch 91, Train Loss: 0.2153, Train Acc: 0.8964, Val Loss: 0.2333, Val Acc: 0.8838\n",
      "Epoch 92, Train Loss: 0.2151, Train Acc: 0.8974, Val Loss: 0.2338, Val Acc: 0.8838\n",
      "Epoch 93, Train Loss: 0.2149, Train Acc: 0.8976, Val Loss: 0.2329, Val Acc: 0.8853\n",
      "Epoch 94, Train Loss: 0.2146, Train Acc: 0.8981, Val Loss: 0.2329, Val Acc: 0.8849\n",
      "Epoch 95, Train Loss: 0.2141, Train Acc: 0.8982, Val Loss: 0.2313, Val Acc: 0.8908\n",
      "Epoch 96, Train Loss: 0.2139, Train Acc: 0.8978, Val Loss: 0.2316, Val Acc: 0.8860\n",
      "Epoch 97, Train Loss: 0.2136, Train Acc: 0.8981, Val Loss: 0.2319, Val Acc: 0.8842\n",
      "Epoch 98, Train Loss: 0.2134, Train Acc: 0.8972, Val Loss: 0.2313, Val Acc: 0.8853\n",
      "Epoch 99, Train Loss: 0.2130, Train Acc: 0.8983, Val Loss: 0.2303, Val Acc: 0.8846\n",
      "\n",
      "Minibatch SGD Test Results:\n",
      "Accuracy: 0.9012\n",
      "F1-Score: 0.9056\n",
      "\n",
      "==================================================\n",
      "COMPARISON OF OPTIMIZATION METHODS\n",
      "==================================================\n",
      "Adam + Minibatch Accuracy: 0.9024\n",
      "SGD (Stochastic) Accuracy: 0.9001\n",
      "Minibatch SGD Accuracy: 0.9012\n"
     ]
    }
   ],
   "source": [
    "# Load data (assuming the data file exists)\n",
    "with open(\"diabetes-processed.csv\", \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_cross_val, y_cross_val, _, _ = preprocess_data(data)\n",
    "\n",
    "# Example 1: Using Adam with Minibatch\n",
    "print(\"=\"*50)\n",
    "print(\"TRAINING WITH ADAM + MINIBATCH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [16, 8]  # Example architecture\n",
    "\n",
    "# Create MLP with Adam optimizer and minibatch gradient descent\n",
    "adam_mlp = MLP(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    output_size=1,\n",
    "    optimizer='adam',          # Specify 'adam' as the optimizer\n",
    "    learning_rate=0.001,       # Lower learning rate often works better with Adam\n",
    "    gdescent='minibatch',      # Use minibatch gradient descent\n",
    "    beta1=0.9,                 # Adam parameter (default)\n",
    "    beta2=0.999,               # Adam parameter (default)\n",
    "    epsilon=1e-8               # Adam parameter (default)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "adam_mlp.train(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    epochs=100,                # Maximum epochs (early stopping may terminate earlier)\n",
    "    batch_size=32,             # Minibatch size\n",
    "    patience=5,                # Early stopping patience\n",
    "    validation_split=0.2       # Portion of training data to use for validation\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "adam_results = adam_mlp.evaluate(X_test, y_test)\n",
    "print(\"\\nAdam + Minibatch Test Results:\")\n",
    "print(f\"Accuracy: {adam_results['accuracy']:.4f}\")\n",
    "print(f\"F1-Score: {adam_results['f1_score']:.4f}\")\n",
    "\n",
    "# Example 2: Using SGD (Stochastic Gradient Descent)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING WITH SGD\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create MLP with SGD (no optimizer specified) \n",
    "sgd_mlp = MLP(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    output_size=1,\n",
    "    optimizer=None,            # Set to None to use default SGD\n",
    "    learning_rate=0.01,        # Usually higher learning rate for SGD\n",
    "    gdescent='stochastic'      # Use stochastic gradient descent (one sample at a time)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "sgd_mlp.train(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    epochs=100,\n",
    "    batch_size=1,              # Ignored for stochastic gradient descent\n",
    "    patience=5,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "sgd_results = sgd_mlp.evaluate(X_test, y_test)\n",
    "print(\"\\nSGD Test Results:\")\n",
    "print(f\"Accuracy: {sgd_results['accuracy']:.4f}\")\n",
    "print(f\"F1-Score: {sgd_results['f1_score']:.4f}\")\n",
    "\n",
    "# Example 3: Using Minibatch SGD without Adam\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING WITH MINIBATCH SGD (WITHOUT ADAM)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create MLP with minibatch SGD but no Adam\n",
    "minibatch_sgd_mlp = MLP(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    output_size=1,\n",
    "    optimizer=None,            # Set to None for vanilla SGD\n",
    "    learning_rate=0.01,\n",
    "    gdescent='minibatch'       # Use minibatch gradient descent\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "minibatch_sgd_mlp.train(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    patience=5,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "minibatch_sgd_results = minibatch_sgd_mlp.evaluate(X_test, y_test)\n",
    "print(\"\\nMinibatch SGD Test Results:\")\n",
    "print(f\"Accuracy: {minibatch_sgd_results['accuracy']:.4f}\")\n",
    "print(f\"F1-Score: {minibatch_sgd_results['f1_score']:.4f}\")\n",
    "\n",
    "# Compare the results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON OF OPTIMIZATION METHODS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Adam + Minibatch Accuracy: {adam_results['accuracy']:.4f}\")\n",
    "print(f\"SGD (Stochastic) Accuracy: {sgd_results['accuracy']:.4f}\")\n",
    "print(f\"Minibatch SGD Accuracy: {minibatch_sgd_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics including error measurements.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    # Classification metrics\n",
    "    TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    TN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    \n",
    "    accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    \n",
    "    # Error metrics\n",
    "    mse = np.mean(np.square(y_true - y_pred))\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse\n",
    "    }\n",
    "\n",
    "def create_baseline_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Creates and evaluates a simple baseline model using majority class prediction.\n",
    "    \"\"\"\n",
    "    # Find the majority class in the training set\n",
    "    majority_class = round(np.mean(y_train))\n",
    "    \n",
    "    # Predict the majority class for all test samples\n",
    "    predictions = np.full(y_test.shape, majority_class)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(y_test, predictions)\n",
    "    \n",
    "    print(\"\\nBaseline Model Results (Majority Class):\")\n",
    "    print(f\"Majority Class: {majority_class}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"MSE: {metrics['mse']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
